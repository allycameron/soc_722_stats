---
title: "SR 8"
author: "Allyson Cameron"
date: "2022-10-25"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**8E1. For each of the causal relationships below, name a hypothetical third variable that would lead to an interaction effect.**\

1.  Bread dough rises because of yeast.\
    Alcohol has a relationship with amount of yeast in the dough, the two together also impact the dough. Alcohol thus also is another reason that dough may rise.\

```{=html}
<!-- -->
```
(1) Education leads to higher income.\
    Gender can also lead to a higher income because it makes the return for education higher for men versus women.\
(2) Gasoline makes a car go.\
    Engine oil and gasoline have a relationship to both impacting the functioning of the car. Low quality engine oil while having gasoline create a different functioning car then a car with a high quality engine oil. Engine oil is thus also another reason that a car functions. \

**8E2. Which of the following explanations invokes an interaction?**\
(1) Caramelizing onions requires cooking over low heat and making sure the onions do not dry out.\
(2) A car will go faster when it has more cylinders or when it has a better fuel injector.\
(3) Most people acquire their political beliefs from their parents, unless they get them instead from their friends.\
(4) Intelligent animal species tend to be either highly social or have manipulative appendages (hands, tentacles, etc.).\

1 is a invokes an interaction because heat *and* making sure the onions do not dry out both impact if the onions are caramelized, but may also have an interaction on each other. 2 and 4 are not interactions because occur separately but because of the *or* it implies that there is probably not a relationship that occurs at the same time and also have some sort of interaction between them. 3 is also not an interaction because it seems like it is an either or with no type of relationship.

**8E3. For each of the explanations in 8E2, write a linear model that expresses the stated relationship.**\

$$
(1)
\begin{align}
C_i &\sim  Normal(\mu_i,\sigma)\\
\mu_i &= \alpha + \beta_H H_i + \beta_D D_i + \beta_{HD}H_iD_i
\end{align}
$$

$$
(2)
\begin{align}
S_i &\sim Normal(\mu_i,\sigma)\\
\mu_i &= \alpha + \beta_CC_i + \beta_FF_i 
\end{align}
$$

$$
(3)
\begin{align}
P_i &\sim Normal(\mu_i,\sigma)\\
\mu_i &= \alpha + \beta_PP_i + \beta_FF_i 
\end{align}
$$

$$
(4)
\begin{align}
I_i &\sim Normal(\mu_i,\sigma)\\
\mu_i &= \alpha + \beta_SS_i + \beta_MM_i
\end{align}
$$

**8M1. Recall the tulips example from the chapter. Suppose another set of treatments adjusted the temperature in the greenhouse over two levels: cold and hot. The data in the chapter were collected at the cold temperature. You find none of the plants grown under the hot temperature developed any blooms at all, regardless of the water and shade levels. Can you explain this result in terms of interactions between water, shade, and temperature?**

In this scenario, we want to make shade and water are dependent on temperature. In other words, even with shade and water they would have no effect without the right temperature. To show this, in the equation when they do not interact with temperature they will be assumed as 0. Temperature is a binary indicator and hot = 0 and cold = 1. This way, when we add across all of these terms thus far when looking at hot (0) then the value is still 0. We will also make our $\alpha$ 0.

$$
\begin{align}
B_i &\sim Normal(\mu_i,\sigma)\\
\mu_i &= \alpha + \beta_TT_i+ \beta_WW_i + \beta_SS_i + \beta_{TW}T_iW_i + \beta_{TS}T_iS_i + \beta_{SW}S_iW_i + \beta_{TSW}T_iS_iW_i
\end{align}
$$

**8M2. Can you invent a regression equation that would make the bloom size zero, whenever the temperature is hot?**

I will just copy my equation I worked out from above. $$
\begin{align}
B_i &\sim Normal(\mu_i,\sigma)\\
\mu_i &= \alpha + \beta_TT_i+ \beta_WW_i + \beta_SS_i + \beta_{TW}T_iW_i + \beta_{TS}T_iS_i + \beta_{SW}S_iW_i + \beta_{TSW}T_iS_iW_i
\end{align}
$$

**8M3. In parts of North America, ravens depend upon wolves for their food. This is because ravens are carnivorous but cannot usually kill or open carcasses of prey. Wolves however can and do kill and tear open animals, and they tolerate ravens co-feeding at their kills. This species relationship is generally described as a "species interaction." Can you invent a hypothetical set of data on raven population size in which this relationship would manifest as a statistical interaction? Do you think the biological interaction could be linear? Why or why not?**

```{r}

library(rethinking)
library(tidybayes.rethinking)
library(ggplot2)
library(dplyr)
library(tidybayes)

# first let's create fake data? 
# working like we did with other simulations and 
# making them build off of each other


bP <- 0.3 
bPW <- 0.5
bW <- 0.2 

N <- 1e4
set.seed(1027)
r <- tibble(
  prey = rnorm(N, mean = 100, sd = 10),
  wolves = rnorm( N, 60, sd = 20),
  ravens = rnorm(N, 5 + bW * wolves + bP * prey + bPW * prey * wolves, sd = 20)) 
# now, let's make our plot.
plot(ravens ~ prey, data = r)
plot(ravens ~ wolves, data = r)



# now lets see the model

m8.3 <- quap(
  alist(
    ravens ~ dnorm(mu, sigma),
    mu <- a + bW * wolves + bP * prey + bPW * wolves * prey,
    a ~ dnorm(5, 1),
    bW ~ dnorm(0, 10),
    bP ~ dnorm(0, 10),
    bPW ~ dnorm(0, 10),
    sigma ~ dexp(1)
  ),
  data = r)

precis(m8.3)
```

All of the parameters are linear regardless of what we add into the model by data. The parameter effects are not dependent upon each other they are all independent which means it is linear. My plot shows that there is no visible effect for prey and ravens but that there is a more visible effect for wolves and ravens which makes sense based on the question. Together with all of the variables there will be an effect.

**8M4. Repeat the tulips analysis, but this time use priors that constrain the effect of water to be positive and the effect of shade to be negative. Use prior predictive simulation. What do these prior assumptions mean for the interaction prior, if anything?**

```{r}
# start loading data

data(tulips)
d <- tibble(tulips)

# let's re-scale like the book
d <- d %>% 
  mutate(blooms_std = blooms/max(blooms)) %>% 
  mutate(water_cent = water - mean(water)) %>% 
  mutate(shade_cent = shade - mean(shade))

# next the model and prior that match the question
m8.4a <- quap(
    alist(
        blooms_std ~ dnorm( mu , sigma ) ,
        mu <- a + Bw * water_cent + Bs * shade_cent + Bws * water_cent * shade_cent,
        a ~ dnorm( 0.5 , 0.25 ) ,
        Bw ~ dnorm(0 , 0.25 ) ,
        Bs ~ dnorm( -2 , 0.25 ) ,
        Bws ~ dnorm(0, 1),
        sigma ~ dexp(1)
) , data = d )

  
set.seed(1027)
prior = extract.prior(m8.4a)


par(mfrow = c(1, 3)) # 3 plots in 1 row
for (s in -1:1) { 
  idx <- which(d$shade_cent == s)
  plot(x = d$water_cent[idx], y = d$blooms_std[idx],
    xlim = c(-1, 1), ylim = c(-4, 4),
    xlab = "water", ylab = "blooms", pch = 16, col = rangi2
  )
  mu <- link(m8.4a, post = prior, data = data.frame(shade_cent = s, 
                                                    water_cent = -1:1))
  for (i in 1:20) lines(-1:1, mu[i, ], col = col.alpha("black", 0.3))
}
 

```

When you change the prior assumptions you can see here that the data flips on itself. This is illustrating how a negative prior effectively constrains the direction that the interaction is able to take.
