---
title: "Chapter 4"
author: "Allyson Cameron"
date: "2022-09-28"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
#### "Easy" Questions\

**4E1. In the model definition below, which line is the likelihood?**\
<div align="center">$y_i$ ∼ Normal(µ, σ)\
µ ∼ Normal(0, 10)\
σ ∼ Exponential(1)\ </div>

The first line $y_i$ ∼ Normal(µ, σ) is the likelihood. 

**4E2. In the model definition just above, how many parameters are in the posterior distribution?**\

There are two parameters: µ and σ. The likelihood is not a parameter.

**4E3. Using the model definition above, write down the appropriate form of Bayes’ theorem that includes the proper likelihood and priors.**

For this, we use the equation we see on page 84.\ 

Pr(µ,σ|y) = $∏_i Normal(y_i|µ, σ)Normal(µ|0, 10)Exponential(σ|1)\above{1pt}∫∫∏_i Normal(y_i|µ, σ)Normal(µ|0, 10)Exponential(σ|1)$

**4E4. In the model definition below, which line is the linear model?**\
<div align="center"> $y_i$ ∼ Normal(µ, σ)\
$µ_i$ = α + $βx_i$\
α ∼ Normal(0, 10)\
β ∼ Normal(0, 1)\
σ ∼ Exponential(2)\ </div>

$µ_i$ = α + $βx_i$ is the linear model.\ 

**4E5. In the model definition just above, how many parameters are in the posterior distribution?**\

There are three parameters above: α, β, and σ. The linear model and the likelihood are not parameters.\  

#### Medium Questions\

**4M1. For the model definition below, simulate observed y values from the prior (not the posterior).**
<div align="center">$y_i$ ∼ Normal(µ, σ)\
µ ∼ Normal(0, 10)\
σ ∼ Exponential(1)\ </div>

Time for some code. 

```{r}
# load packages
library(tidyverse)
library(rethinking)

# to simulate for y we will use the code on page 82.

# create how many samples we want and set seed
n <- 1e4
set.seed(0928)

# create a tibble with all information needed
sim <-
  tibble(
    sample_mu = rnorm(n, 0, 10),
    sample_sigma = rexp(n, 1)
  ) %>%
  mutate(prior_y = rnorm(n, sample_mu, sample_sigma))

# plot

ggplot(sim, aes(x = prior_y)) +
  geom_density() +
  labs(title = "observed y ~ Normal(µ, σ)", x = "Prior Y")
```

**4M2. Translate the model just above into a `quap` formula.**
```{r}
# now we need to use the alist() function and turn this into a quap formula

quap_form <- alist(
  y ~ dnorm(mu, sigma),
  mu ~ dnorm(0, 10),
  sigma ~ dexp(1)
)
```

**4M3. Translate the quap model formula below into a mathematical model definition.**
<div align="center"> y ~ dnorm( mu , sigma ), \
mu <- a + b*x,\
a ~ dnorm( 0 , 10 ), \
b ~ dunif( 0 , 1 ), \
sigma ~ dexp( 1 ) \ </div>


This would be written as:\
 $y_i$ ~ Normal(µ, σ) \
$µ_i$ = $α + βx_i$\
α ~ Normal(0, 10)\
β ~ Uniform(0, 1)\
σ ~ Exponential(1)\

**4M4. A sample of students is measured for height each year for 3 years. After the third year, you want to fit a linear regression predicting height using year as a predictor. Write down the mathematical model definition for this regression, using any variable names and priors you choose. Be prepared to defend your choice of priors.**


Let's start by defining our likelihood and linear model. We will use $y_i$ to notate year and $h_i$ to notate height. \
$h_i$ ~ Normal(µ, σ)\
$µ_i$ = $α + β(y_i)$\

Now, the priors. \

For α, we will use a normal distribution because we learned that height is usually normally distributed. We don't know exactly what grades we have for student's so let's just use an average height for boys and girls. 

I found that from the age of 10 to 12, girls are on average around 55 to 66 inches (approx. 140 to 168 cm; mid-range = approx. 154) and boys are around 54 to 67 inches (approx. 137 to 170 cm; mid-range = approx. 154 cm). From this, I choose to use my µ value in α as 154. I will use a high standard deviation of 20 cm to account for 1) that this is a rough estimate and 2) that I do not know the age of the students.
\
Information from: https://www.livestrong.com/article/276954-normal-height-weight-for-a-school-age-child/. \

α ~ Normal(154, 20)\ 

β is telling us growth per year(slope). We'll make the average growth rate 0 but allow a 10 cm standard deviation. 10 cm is a little bit more than the average growth rate for children which is 6 cm. I wanted to pick a number that would be easier to work with in the next part (I peaked and see we'll probably use log-norm then). 

β ~ Normal(0, 10)\

Lastly, let's look at our σ value. Like in the video, we do not have a very informed prior. So we will use a large number to account for our uncertainty as the spread (variation). Let's choose 20 like we did for alpha.

σ ~ Uniform(0, 20)\


Overall we decided:\
$h_i$ ~ Normal(µ, σ) \
$µ_i$ = $α + β(y_i)$\
α ~ Normal(154, 20)\
β ~ Normal(0, 10)\
σ ~ Uniform(0, 20)\


**4M5. Now suppose I remind you that every student got taller each year. Does this information lead you to change your choice of priors? How?**\
Since now I know that we *are* supposed to keep in mind that students are growing every year. I will revise my prior in relation to rate of growth. 

In the book we used a logarithmic distribution to solve this, so I will do the same. 

Overall we decided:\
$h_i$ ~ Normal(µ, σ) \
$µ_i$ = $α + β(year_i)$\
α ~ Normal(154, 20)\
β ~ Log-Normal(0, 1)\
σ ~ Uniform(0, 20)\

**4M6. Now suppose I tell you that the variance among heights for students of the same age is never more than 64cm. How does this lead you to revise your priors?**\

Since now we are discussing variance this leads me to inspect my sigma value. Sigma is our standard deviation which is the square root of variance. We will replace our estimated standard deviation with 8. 

Overall we decided:\
$h_i$ ~ Normal(µ, σ) \
$µ_i$ = $α + β(year_i)$\
α ~ Normal(154, 20)\
β ~ Log-Normal(0, 1)\
σ ~ Uniform(0, 8)\

**4M7. Refit model m4.3 from the chapter, but omit the mean weight x-bar this time. Compare the new model’s posterior to that of the original model. In particular, look at the covariance among the parameters. What is different? Then compare the posterior predictions of both models.**
```{r}
# 4.42 load data
library(rethinking)
library(tidybayes)
library(tidybayes.rethinking)
library(patchwork)
data(Howell1)
d <- Howell1
d2 <- d %>%
  filter(age >= 18)
# define the average weight, x-bar
d2 <- d2 %>%
  mutate(
    xbar = mean(weight),
    cweight = weight - xbar)

# fit model
flist1 <- alist(
  height ~ dnorm(mu, sigma),
  mu <- a + b * cweight,
  a ~ dnorm(178, 20),
  b ~ dlnorm(0, 1),
  sigma ~ dunif(0, 50)
)

m4.3 <- quap(flist1, data = d2)

precis(m4.3)

# now let's make our plot

# take our samples
draws <- tidy_draws(m4.3, n = 100)
head(draws)

# actually create plot
p <- ggplot(draws) +
  geom_abline(aes(
    intercept = a,
    slope = b
  ),
  alpha = .2
  ) +
  geom_point(
    data = d2,
    mapping = aes(
      x = cweight,
      y = height
    ),
    alpha = .2
  ) +
  labs(
    x = "weight - mean(weight) in kg",
    y = "height in cm",
    title = "Posterior estimates & original data",
    subtitle = "centered at the mean weight"
  )

# now let's work on adding sigma
ppsims <- predicted_draws(m4.3,
  newdata = d2,
  draws = 1000
)
head(ppsims, 20)

ppsims <- ppsims %>%
  group_by(.row) %>%
  mutate(
    lo_bound = HPDI(.prediction)[1],
    up_bound = HPDI(.prediction)[2]
  )

# dun, dun, dun dun ... the first plot
p <- p + geom_ribbon(
  data = ppsims,
  mapping = aes(
    x = cweight,
    ymax = up_bound,
    ymin = lo_bound
  ),
  alpha = .1
) +
  labs(caption = "with 89% HPDI overlaid")

# now without x-bar
m4.3b <- quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b * (d2$weight),
    a ~ dnorm(178, 20),
    b ~ dlnorm(0, 1),
    sigma ~ dunif(0, 50)
  ),
  data = d2)

precis(m4.3b)

# now let's make our second plot

# take our samples
draws_b <- tidy_draws(m4.3b, n = 100)
head(draws_b)

# actually create plot
p_b <- ggplot(draws_b) +
  geom_abline(aes(
    intercept = a,
    slope = b), alpha = .2) +
  geom_point(
    data = d2,
    mapping = aes(
      x = weight,
      y = height
    ),
    alpha = .2
  ) +
  labs(
    x = "weight in kg",
    y = "height in cm",
    title = "Posterior estimates & original data"
  )

# now let's work on adding sigma
ppsims_b <- predicted_draws(m4.3b,
  newdata = d2,
  draws = 1000
)
head(ppsims_b, 20)

ppsims_b <- ppsims_b %>%
  group_by(.row) %>%
  mutate(
    lo_bound = HPDI(.prediction)[1],
    up_bound = HPDI(.prediction)[2]
  )

# dun, dun, dun dun ... the second plot
p_b <- p_b + geom_ribbon(
  data = ppsims_b,
  mapping = aes(
    x = weight,
    ymax = up_bound,
    ymin = lo_bound
  ),
  alpha = .1
) +
  labs(caption = "with 89% HPDI overlaid")

p + p_b
# comparing the plots

# let's look at the covariance among parameters.

# with x-bar
round(cov2cor(vcov(m4.3)), 3)
# without x-bar
round(cov2cor(vcov(m4.3b)), 3)
```
The graph centered at the mean has a starting value of -10 while the graph not centered at the mean starts at 30 kg. Basically, the mean of around 50 is not our center point of 0. The slope and lines are still the same. The x-axis is notating the difference from the mean instead of values of 40, 59, 69, etc. So 10 is saying those individuals are 10 kg from the mean (which is approx. 50).

Looking at the covariance among the parameters, they are pretty similar. The non-centered mean has a greater covariance but the direction of association for each relationship is the same.

**4M8. In the chapter, we used 15 knots with the cherry blossom spline. Increase the number of knots and observe what happens to the resulting spline. Then adjust also the width of the prior on the weights—change the standard deviation of the prior and watch what happens. What do you think the combination of knot number and the prior on the weights controls?**

```{r}
library(splines)

# load in data
data(cherry_blossoms)
d <- cherry_blossoms
precis(d)

# create new data frame
d2 <- d[complete.cases(d$doy), ]
num_knots <- 15
num_knots_c <- 30
knot_list <- quantile(d2$year, probs = seq(0, 1, length.out = num_knots))
knot_list_c <- quantile(d2$year, probs = seq(0, 1, length.out = num_knots_c))

B <- bs(d2$year,
  knots = knot_list[-c(1, num_knots)],
  degree = 3, intercept = TRUE
)

C <- bs(d2$year,
  knots = knot_list_c[-c(1, num_knots_c)],
  degree = 3, intercept = TRUE
)

# plot the two

plot(NULL, xlim = range(d2$year), ylim = c(0, 1), 
     xlab = "year", ylab = "basis")
for (i in 1:ncol(B)) lines(d2$year, B[, i])

plot(NULL, xlim = range(d2$year), ylim = c(0, 1), 
     xlab = "year", ylab = "basis")
for (i in 1:ncol(C)) lines(d2$year, C[, i])

# build the models
# let's first plot the larger number of knots with each size sigma

m4.7c_a <- quap(
  alist(
    D ~ dnorm(mu, sigma),
    mu <- a + C %*% w,
    a ~ dnorm(100, 10),
    w ~ dnorm(0, 10),
    sigma ~ dexp(1)
  ),
  data = list(D = d2$doy, C = C),
  start = list(w = rep(0, ncol(C)))
)

post <- extract.samples(m4.7c_a)
w <- apply(post$w, 2, mean)
plot(NULL,
  xlim = range(d2$year), ylim = c(-6, 6),
  xlab = "year", ylab = "basis * weight", main = "sigma ~ dexp(1) and 30 knots")
for (i in 1:ncol(C)) lines(d2$year, w[i] * C[, i])


m4.7c <- quap(
  alist(
    D ~ dnorm(mu, sigma),
    mu <- a + C %*% w,
    a ~ dnorm(100, 10),
    w ~ dnorm(0, 10),
    sigma ~ dexp(250)
  ),
  data = list(D = d2$doy, C = C),
  start = list(w = rep(0, ncol(C))))


post_c <- extract.samples(m4.7c)
w2 <- apply(post_c$w, 2, mean)
plot(NULL,
  xlim = range(d2$year), ylim = c(-6, 6),
  xlab = "year", ylab = "basis * weight", main = "sigma ~ dexp(250) and 30 knots")
for (i in 1:ncol(C)) lines(d2$year, w2[i] * C[, i])



# now let's do the same thing but with the smaller number of knots to compare. 

m4.7d_a <- quap(
  alist(
    D ~ dnorm(mu, sigma),
    mu <- a + B %*% w,
    a ~ dnorm(100, 10),
    w ~ dnorm(0, 10),
    sigma ~ dexp(1)
  ),
  data = list(D = d2$doy, B = B),
  start = list(w = rep(0, ncol(B)))
)

post_d_a <- extract.samples(m4.7d_a)
w <- apply(post_d_a$w, 2, mean)
plot(NULL,
  xlim = range(d2$year), ylim = c(-6, 6),
  xlab = "year", ylab = "basis * weight", main = "sigma ~ dexp(1) and 15 knots")
for (i in 1:ncol(B)) lines(d2$year, w[i] * B[, i])


m4.7d <- quap(
  alist(
    D ~ dnorm(mu, sigma),
    mu <- a + B %*% w,
    a ~ dnorm(100, 10),
    w ~ dnorm(0, 10),
    sigma ~ dexp(250)
  ),
  data = list(D = d2$doy, B = B),
  start = list(w = rep(0, ncol(B))))


post_d <- extract.samples(m4.7d)
w2 <- apply(post_d$w, 2, mean)
plot(NULL,
  xlim = range(d2$year), ylim = c(-6, 6),
  xlab = "year", ylab = "basis * weight", main = "sigma ~ dexp(250) and 15 knots")
for (i in 1:ncol(B)) lines(d2$year, w2[i] * B[, i])




```

The increase in knots made more places for the pivots to occur. So now our line is wigglier (as in the scientific term lol). Additionally, looking at the four graphs I made, when sigma is the same and knot value is change, there peaks of each curve seem higher and there are more of them. I cannot make out a huge difference in the change in the sigma value. At 30 knots, it seems like the sigma change impacted the where the first negative peaks placement is (dexp(1) = approaching -6, dexp(250) past -6).